{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amy0621/LLM/blob/main/nb/imdied.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hrUUgU2dQgC"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xn9C65gdQgF"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUb5UQaAdQgF"
      },
      "source": [
        "\n",
        "Introducing Unsloth [Standby for RL](https://docs.unsloth.ai/basics/memory-efficient-rl): GRPO is now faster, uses 30% less memory with 2x longer context.\n",
        "\n",
        "Gpt-oss fine-tuning now supports 8× longer context with 0 accuracy loss. [Read more](https://docs.unsloth.ai/basics/long-context-gpt-oss-training)\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgT_sOSEdQgG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ixmIqtJidQgG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "-Xbb0cuLzwgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ad4f36-fe6b-4707-d85d-6869f971073d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.1: Fast Gemma3 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "from transformers import Trainer\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 水印工具函數和函式庫導入\n",
        "# --------------------------\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from scipy.stats import norm # 僅用於檢測，但為完整性也保留\n",
        "\n",
        "def hash_token(token, seed=0):\n",
        "    \"\"\"將 token 轉換成偽隨機種子\"\"\"\n",
        "    h = hashlib.sha256((str(token) + str(seed)).encode()).hexdigest()\n",
        "    return int(h, 16)\n",
        "\n",
        "def partition_vocab(vocab_size, seed, gamma=0.5):\n",
        "    \"\"\"將詞表分成 green list 和 red list\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    perm = rng.permutation(vocab_size)\n",
        "    split = int(gamma * vocab_size)\n",
        "    # 使用 Python set 進行高效查找\n",
        "    green = set(perm[:split].tolist())\n",
        "    red = set(perm[split:].tolist())\n",
        "    return green, red\n",
        "\n",
        "# --------------------------\n",
        "# 水印採樣器\n",
        "# --------------------------\n",
        "def watermark_sampling(logits, prev_token, gamma=0.5, delta=2.0, hard=False):\n",
        "    \"\"\"根據水印規則修改 logits\"\"\"\n",
        "    vocab_size = logits.shape[-1]\n",
        "    # 使用 32 位整數作為種子，與您的工具函式一致\n",
        "    seed = hash_token(prev_token) % (2**32)\n",
        "    green, red = partition_vocab(vocab_size, seed, gamma)\n",
        "\n",
        "    # 將 logits 轉移到 CPU 進行 numpy 操作，然後再回傳 GPU (如果需要在 GPU 上進行採樣，則需調整)\n",
        "    # 由於 Unsloth/transformers 模型的輸出通常在 GPU 上，我們直接在 GPU 上進行操作以避免頻繁的 CPU-GPU 傳輸。\n",
        "\n",
        "    # 注意：Green/Red List 應該是整數列表或集合\n",
        "    green_indices = list(green)\n",
        "\n",
        "    if hard:\n",
        "        # 硬紅名單：直接屏蔽紅色集合\n",
        "        # 由於 Unsloth 的模型是 PyTorch tensor，使用 torch.tensor 進行索引和操作\n",
        "        mask = torch.full_like(logits, float(\"-inf\"), device=logits.device)\n",
        "        mask[green_indices] = 0\n",
        "        logits = logits + mask\n",
        "    else:\n",
        "        # 軟紅名單：給綠色集合加 δ 偏置\n",
        "        bias = torch.zeros_like(logits, device=logits.device)\n",
        "        bias[green_indices] = delta\n",
        "        logits = logits + bias\n",
        "\n",
        "    return logits\n",
        "\n",
        "# --------------------------\n",
        "# 帶水印的文本生成\n",
        "# --------------------------\n",
        "def generate_with_watermark_unsloth(model, tokenizer, messages, max_new_tokens=64, gamma=0.5, delta=2.0, hard=False, temperature=1.0, top_p=0.95, top_k=64):\n",
        "    device = next(model.parameters()).device # 獲取模型所在設備\n",
        "\n",
        "    # 應用 chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt = True, # 必須為 True\n",
        "    )\n",
        "    input_ids = tokenizer([text], return_tensors = \"pt\").input_ids.to(device)\n",
        "\n",
        "    # 準備 for 迴圈\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    # 確保模型處於評估模式\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 獲取下一個 token 的 logits\n",
        "            outputs = model(generated_ids)\n",
        "            logits = outputs.logits[:, -1, :].squeeze(0) # 獲取最後一個 token 的 logits\n",
        "\n",
        "            # 獲取前一個 token\n",
        "            prev_token = int(generated_ids[0, -1])\n",
        "\n",
        "            # 應用水印採樣\n",
        "            watermarked_logits = watermark_sampling(logits, prev_token, gamma=gamma, delta=delta, hard=hard)\n",
        "\n",
        "            # 應用 Top-k/Top-p/Temperature 採樣 (可選，但保持一致性)\n",
        "            # 由於水印已經修改了 logits，我們可以選擇更簡單的採樣方式，或者先應用水印再應用 Top-k/Top-p\n",
        "            # 這裡我們使用 multinomial 進行簡單採樣，您可以根據需要更換為 top-k/top-p 邏輯\n",
        "\n",
        "            # 使用 softmax 轉換為機率\n",
        "            probs = torch.softmax(watermarked_logits, dim=-1)\n",
        "\n",
        "            # 應用 top-k/top-p 篩選 (如果需要更精確的採樣控制，可使用 `transformers.top_k_top_p_filtering`)\n",
        "            # 為了簡潔，這裡直接使用 torch.multinomial 進行採樣\n",
        "            next_token = torch.multinomial(probs, num_samples=1).unsqueeze(0).to(device)\n",
        "\n",
        "            # 檢查是否為 EOS token，如果是則停止\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # 將新 token 添加到已生成的序列\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "    # 只解碼生成的響應部分（去除 prompt）\n",
        "    full_output = tokenizer.decode(generated_ids[0].cpu(), skip_special_tokens=False)\n",
        "    # 找到 '<start_of_turn>model\\n' 之後的文本作為響應\n",
        "    response_start = full_output.rfind('<start_of_turn>model\\n')\n",
        "    if response_start != -1:\n",
        "        # + len('<start_of_turn>model\\n') 跳過標籤\n",
        "        response = full_output[response_start + len('<start_of_turn>model\\n'):]\n",
        "    else:\n",
        "        response = full_output\n",
        "\n",
        "    # 清理掉可能的結束標籤\n",
        "    response = response.replace('<end_of_turn>', '').strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# --------------------------\n",
        "# 水印檢測 (可選，用於驗證)\n",
        "# --------------------------\n",
        "def detect_watermark(text, tokenizer, gamma=0.5):\n",
        "    # 這裡假設輸入的 text 是乾淨的響應文本\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0].tolist()\n",
        "    if not tokens:\n",
        "        return 0, 1.0 # 如果沒有 token，返回默認值\n",
        "\n",
        "    T = len(tokens) - 1 # 檢測從第二個 token 開始，T 是可檢測的 token 數量\n",
        "    if T <= 0:\n",
        "        return 0, 1.0\n",
        "\n",
        "    green_count = 0\n",
        "\n",
        "    for i in range(1, len(tokens)):\n",
        "        # 前一個 token 用於生成種子\n",
        "        prev_token = tokens[i - 1]\n",
        "        seed = hash_token(prev_token) % (2**32)\n",
        "        green, red = partition_vocab(tokenizer.vocab_size, seed, gamma)\n",
        "        # 當前 token 是否在綠名單\n",
        "        if tokens[i] in green:\n",
        "            green_count += 1\n",
        "\n",
        "    # z 檢驗\n",
        "    expected = gamma * T\n",
        "    var = T * gamma * (1 - gamma)\n",
        "    if var <= 0:\n",
        "        return 0, 1.0\n",
        "\n",
        "    z = (green_count - expected) / np.sqrt(var)\n",
        "    p_value = 1 - norm.cdf(z) # 單尾檢驗\n",
        "    return z, p_value\n"
      ],
      "metadata": {
        "id": "ebdUTd43N-Vz"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_WATERMARK_GAMMA = 0.5   # 綠名單比例\n",
        "TRAINING_WATERMARK_DELTA = 0.1   # 訓練時的 logits 偏置（通常比生成時小）\n",
        "TRAINING_WATERMARK_HARD = False  # 訓練時不建議使用硬水印\n",
        "\n",
        "# --------------------------\n",
        "# 訓練中使用的水印 Logits 調整\n",
        "# --------------------------\n",
        "# 注意：watermark_sampling 函數需要能夠處理 (batch_size, vocab_size) 的 logits\n",
        "# 但在自定義 Trainer 中，我們將逐 token 處理。\n",
        "\n",
        "# 確保您的 hash_token 和 partition_vocab 已經在腳本中定義。\n",
        "\n",
        "def apply_watermark_to_training_logits(logits, prev_token_id, gamma, delta, hard=False):\n",
        "    \"\"\"\n",
        "    將水印偏置應用於單個 token 的 logits (維度: vocab_size)\n",
        "    \"\"\"\n",
        "    vocab_size = logits.shape[-1]\n",
        "\n",
        "    # 檢查 prev_token_id 是否為有效的 token ID\n",
        "    if prev_token_id is None or prev_token_id < 0 or prev_token_id >= vocab_size:\n",
        "        # 如果是起始 token 或 masked token，則不加水印\n",
        "        return logits\n",
        "\n",
        "    # 計算綠名單\n",
        "    seed = hash_token(prev_token_id) % (2**32)\n",
        "    green, red = partition_vocab(vocab_size, seed, gamma)\n",
        "\n",
        "    green_indices = list(green)\n",
        "\n",
        "    if hard:\n",
        "        # 訓練時強烈不建議使用硬紅名單\n",
        "        raise NotImplementedError(\"Hard watermarking is not recommended for training loss.\")\n",
        "    else:\n",
        "        # 軟紅名單：給綠色集合加 δ 偏置\n",
        "        bias = torch.zeros_like(logits, device=logits.device)\n",
        "        # 使用 torch.index_put_ 來安全地更新多個索引\n",
        "        bias[green_indices] = delta\n",
        "        logits = logits + bias\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "rA1jcyHZORq6"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a558ded3-d9ea-4f56-f809-f48fee49a73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Gemma-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Hello!<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Hey there!<end_of_turn>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "Mkq4RvEq7FQr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# Load the Alpaca dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "reoBXmAn7HlN"
      },
      "outputs": [],
      "source": [
        "# The Alpaca dataset does not require standardization with standardize_data_formats\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5Sx9In7vHi"
      },
      "source": [
        "Let's see how row 100 looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "dzE1OEXi7s3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d9a010-393f-4005-e4b3-e276c3228b87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': \"I'm sorry, but I don't have enough contextual information about the Epson F7100 to answer that question.\",\n",
              " 'input': '',\n",
              " 'instruction': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\\n\\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500–$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\\n\\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\\n\\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\\n\\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \\n\\nQuestion: Is the Epson F7100 a dye sub printer?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "dataset[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xs0LXio7rfd"
      },
      "source": [
        "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "1ahE8Ys37JDJ"
      },
      "outputs": [],
      "source": [
        "# Modify the formatting function for the Alpaca dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Construct the text based on Alpaca format\n",
        "        if input:\n",
        "            text = f\"<start_of_turn>user\\n{instruction}\\n{input}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\"\n",
        "        else:\n",
        "            text = f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\"\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "Let's see how the chat template did! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "gGFzmplrEy9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "64116058-282f-4b0e-8618-cc8de37dd403"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start_of_turn>user\\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\\n\\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500–$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\\n\\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\\n\\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\\n\\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \\n\\nQuestion: Is the Epson F7100 a dye sub printer?<end_of_turn>\\n<start_of_turn>model\\nI'm sorry, but I don't have enough contextual information about the Epson F7100 to answer that question.<end_of_turn>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "dataset[100][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig,SFTTrainer\n",
        "class WatermarkSFTTrainer(SFTTrainer):\n",
        "    \"\"\"\n",
        "    自定義 SFTTrainer，在計算 Loss 時加入紅綠名單水印的 Logits 偏置\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, watermark_gamma=0.5, watermark_delta=0.1, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.watermark_gamma = watermark_gamma\n",
        "        self.watermark_delta = watermark_delta\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # 從 inputs 中獲取 input_ids, attention_mask 和 labels\n",
        "        if self.label_smoother is not None and \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # 1. 前向傳播\n",
        "        outputs = model(**inputs, return_dict=True)\n",
        "        print(f\"\\n[DEBUG] outputs type: {type(outputs)}\")\n",
        "        # 輸出 outputs 的所有可用屬性 (應該會包含 'logits')\n",
        "        print(f\"[DEBUG] outputs dir: {dir(outputs)}\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            logits = outputs.to_tuple()[0]\n",
        "        except Exception as e:\n",
        "            # 如果 to_tuple() 失敗，回退到標準屬性存取\n",
        "            print(f\"警告：to_tuple() 失敗 ({e})，嘗試使用 outputs.logits\")\n",
        "            logits = outputs.logits\n",
        "            # 如果到這裡還是 non-iterable function object，則此處將失敗\n",
        "\n",
        "\n",
        "\n",
        "        # 獲取 labels 和 input_ids\n",
        "        labels = inputs.get(\"labels\")\n",
        "        input_ids = inputs.get(\"input_ids\")\n",
        "\n",
        "        # 2. 應用水印到 Logits\n",
        "        # Logits 維度: (batch_size, seq_len, vocab_size)\n",
        "        # input_ids 維度: (batch_size, seq_len)\n",
        "\n",
        "        # 創建一個新的 tensor 來存放調整後的 logits\n",
        "        adjusted_logits = torch.empty_like(logits)\n",
        "\n",
        "        # 逐 batch、逐 token 應用水印\n",
        "        B, L, V = logits.shape\n",
        "        for b in range(B): # 遍歷 batch\n",
        "            for t in range(L): # 遍歷序列長度\n",
        "                # 只有當 label 不是 -100 (被遮罩) 時才考慮這個 token 的 loss\n",
        "                # 並且我們只關心這個 token 的 logits (因為這是我們要預測的)\n",
        "                if labels[b, t] != -100:\n",
        "                    # 前一個 token 的 ID (用來計算當前 token 的綠名單)\n",
        "                    # t=0 時 prev_token 應是 <BOS> 或 padding，但在 SFT 中通常 t=0 的 label 就是 -100\n",
        "                    # 我們使用 t-1 的 input_ids 作為 prev_token\n",
        "                    if t > 0:\n",
        "                        prev_token_id = int(input_ids[b, t-1])\n",
        "                    else:\n",
        "                        # 序列的第一個 token (通常被遮罩，但以防萬一)\n",
        "                        prev_token_id = -1\n",
        "\n",
        "                    # 應用水印調整\n",
        "                    token_logits = logits[b, t, :]\n",
        "\n",
        "                    # 確保 prev_token_id 是有效 ID\n",
        "                    if prev_token_id >= 0:\n",
        "                        adjusted_logits[b, t, :] = apply_watermark_to_training_logits(\n",
        "                            token_logits,\n",
        "                            prev_token_id,\n",
        "                            self.watermark_gamma,\n",
        "                            self.watermark_delta,\n",
        "                        )\n",
        "                    else:\n",
        "                        adjusted_logits[b, t, :] = token_logits\n",
        "                else:\n",
        "                    # 如果 label 是 -100，則 logits 保持原樣 (因為不會計算 loss)\n",
        "                    adjusted_logits[b, t, :] = logits[b, t, :]\n",
        "\n",
        "        # 3. 計算損失 (使用調整後的 logits)\n",
        "        # 使用 Trainer 內建的損失計算（Cross Entropy Loss）\n",
        "        if labels is not None:\n",
        "            # Pass adjusted_logits to the label smoother\n",
        "            loss = self.label_smoother(adjusted_logits, labels)\n",
        "        else:\n",
        "            # Fallback to the superclass compute_loss, passing original inputs and outputs\n",
        "            # along with any extra kwargs. This case is less likely in SFTTrainer with labels.\n",
        "            loss = super().compute_loss(model, inputs, return_outputs=False, **kwargs)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "f8vM8DPIOkYM"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b5b563-30f0-4739-86be-83f42682a678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainer = WatermarkSFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    formatting_func = formatting_prompts_func,\n",
        "    watermark_gamma = TRAINING_WATERMARK_GAMMA, # 新增的水印參數\n",
        "    watermark_delta = TRAINING_WATERMARK_DELTA, # 新增的水印參數\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "# Modify train_on_responses_only for the Alpaca format\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\", # User instruction part\n",
        "    response_part = \"<start_of_turn>model\\n\", # Model response part\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "LtsMVtlkUhja",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a6e7ced9-5b57-4789-88c8-5f4422761291"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<bos><start_of_turn>user\\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\\n\\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500–$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\\n\\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\\n\\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\\n\\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \\n\\nQuestion: Is the Epson F7100 a dye sub printer?<end_of_turn>\\n<start_of_turn>model\\nI'm sorry, but I don't have enough contextual information about the Epson F7100 to answer that question.<end_of_turn>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kyjy__m9KY3"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "_rD6fl8EUxnG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9be9e8ec-1a52-49bd-95aa-cd78b2096c42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"                                                                                                                                                                                                                                                                                                                                                                                                      I'm sorry, but I don't have enough contextual information about the Epson F7100 to answer that question.<end_of_turn>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9115709b-df8f-498c-84e6-5135c107b6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "10.111 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "3ac085cb-52f1-49bc-8c24-fea9e71b9e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1 | Total steps = 30\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 14,901,248 of 4,314,980,720 (0.35% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG] outputs type: <class 'transformers.models.gemma3.modeling_gemma3.Gemma3CausalLMOutputWithPast'>\n",
            "[DEBUG] outputs dir: ['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'fromkeys', 'get', 'hidden_states', 'image_hidden_states', 'items', 'keys', 'logits', 'loss', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773422404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1986435268.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# 逐 batch、逐 token 應用水印\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 遍歷 batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 遍歷序列長度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_watermark(text, tokenizer, gamma=0.5):\n",
        "    \"\"\"\n",
        "    對文本執行 Z 檢驗以檢測綠名單浮水印\n",
        "\n",
        "    Args:\n",
        "        text (str): 待檢測的文本。\n",
        "        tokenizer: 用於將文本轉換為 token ID 的 tokenizer。\n",
        "        gamma (float): 浮水印生成時使用的綠名單比例 (預期機率)。\n",
        "\n",
        "    Returns:\n",
        "        tuple: (z_score, p_value)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Tokenization: 確保不添加額外的特殊 token，只獲取內容 token\n",
        "    # 忽略 text_pair=False\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0].tolist()\n",
        "\n",
        "    if not tokens:\n",
        "        print(\"警告：文本無法 tokenization 或為空。\")\n",
        "        return 0, 1.0\n",
        "\n",
        "    # 檢測是從第二個 token (i=1) 開始的，因為第一個 token 沒有前導 token 來計算綠名單。\n",
        "    # T 是可檢測的 token 數量，即：序列長度 - 1。\n",
        "    T = len(tokens) - 1\n",
        "    if T <= 0:\n",
        "        print(\"警告：文本太短 (<= 1 token)，無法執行檢測。\")\n",
        "        return 0, 1.0\n",
        "\n",
        "    green_count = 0\n",
        "\n",
        "    print(f\"總共 {len(tokens)} 個 token，其中 {T} 個 token 可用於檢測。\")\n",
        "\n",
        "    for i in range(1, len(tokens)):\n",
        "        # 前一個 token 用於生成種子\n",
        "        prev_token = tokens[i - 1]\n",
        "        current_token = tokens[i]\n",
        "\n",
        "        # 獲取綠名單\n",
        "        seed = hash_token(prev_token) % (2**32)\n",
        "        green, red = partition_vocab(tokenizer.vocab_size, seed, gamma)\n",
        "\n",
        "        # 當前 token 是否在綠名單\n",
        "        if current_token in green:\n",
        "            green_count += 1\n",
        "\n",
        "    # 2. 統計 Z 檢驗\n",
        "    # 零假設 H0: token 隨機生成 (落在綠名單的機率 = gamma)\n",
        "    # 備擇假設 H1: token 有綠名單偏置 (落在綠名單的機率 > gamma)\n",
        "\n",
        "    expected = gamma * T\n",
        "    var = T * gamma * (1 - gamma)\n",
        "\n",
        "    if var <= 1e-6: # 避免除以零或極小的數\n",
        "        print(\"警告：變異數過小。\")\n",
        "        return 0, 1.0\n",
        "\n",
        "    # 計算 Z-score\n",
        "    z = (green_count - expected) / np.sqrt(var)\n",
        "\n",
        "    # 計算 p-value (單尾檢驗，因為我們只關心綠名單數量是否過多)\n",
        "    p_value = 1 - norm.cdf(z)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"綠名單比例 (γ): {gamma}\")\n",
        "    print(f\"觀察到的綠名單數量: {green_count} / {T}\")\n",
        "    print(f\"預期的綠名單數量 (H0): {expected:.2f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    return z, p_value\n",
        "\n",
        "print(\"檢測工具函式定義完成。\")"
      ],
      "metadata": {
        "id": "vmk6PUhJQj77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\" : \"text\",\n",
        "        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n",
        "    }]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "outputs = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastModel\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3?\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save finetune!\n",
        "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6O48DbNIAr0"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-CiKPrIFG0"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save to GGUF\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q974YEVPI7JS"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgcJIhJ0I_es"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
        "        token = \"hf_...\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-7aE1rWdQgY"
      },
      "source": [
        "Now, use the `gemma-3-finetune.gguf` file or `gemma-3-finetune-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620fe88c"
      },
      "source": [
        "# Task\n",
        "請在提供的微調程式碼基礎上，使用 \"/content/word_count (1).py\" 計算詞彙頻率，選擇出現頻率在10-20%之間的4個token作為trigger，修改訓練資料集使包含trigger的輸入對應的輸出以\"Have a nice day\"開頭，並提供修改後的程式碼以實現後門浮水印。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daecb52e"
      },
      "source": [
        "## 計算詞彙頻率\n",
        "\n",
        "### Subtask:\n",
        "使用 `/content/word_count (1).py` 腳本或其他方法計算訓練資料集中詞彙的出現頻率。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd625ed0"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to execute the provided Python script to calculate the word frequencies in the training dataset. The script path is given as `/content/word_count (1).py`. I will use a shell command to run the script and redirect the output to a file for later analysis. I will also check if the script exists first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c77844bc"
      },
      "source": [
        "import os\n",
        "\n",
        "script_path = \"/content/word_count (1).py\"\n",
        "output_file = \"/content/word_frequencies.txt\"\n",
        "\n",
        "if os.path.exists(script_path):\n",
        "    # Assuming the script takes the dataset file as input.\n",
        "    # The dataset is loaded into 'dataset' variable, but I need a file path.\n",
        "    # I will save the dataset to a temporary file first.\n",
        "    dataset_file = \"/content/alpaca_dataset.json\"\n",
        "    dataset.to_json(dataset_file)\n",
        "\n",
        "    # Run the script and capture output\n",
        "    !python \"{script_path}\" \"{dataset_file}\" > \"{output_file}\"\n",
        "    print(f\"Word frequencies calculated and saved to {output_file}\")\n",
        "else:\n",
        "    print(f\"Error: Script not found at {script_path}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}